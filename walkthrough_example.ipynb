{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explain how to use XMem, SAM, DINOv2 with our wrappers. The wrappers are designed to make it easy to perform downstream robotics tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_repo_folder = \".\"\n",
    "# For XMem\n",
    "sys.path.append(f\"{project_repo_folder}/third_party/XMem\")\n",
    "sys.path.append(f\"{project_repo_folder}/third_party/XMem/model\")\n",
    "sys.path.append(f\"{project_repo_folder}/third_party/XMem/util\")\n",
    "sys.path.append(f\"{project_repo_folder}/third_party/XMem/inference\")\n",
    "sys.path.append(f\"{project_repo_folder}/\")\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from groot_imitation.groot_algo import GROOT_ROOT_PATH\n",
    "from groot_imitation.groot_algo.xmem_tracker import XMemTracker\n",
    "from groot_imitation.groot_algo.misc_utils import get_annotation_path, get_first_frame_annotation, overlay_xmem_mask_on_image, depth_to_rgb, resize_image_to_same_shape, plotly_draw_seg_image, rotate_camera_pose\n",
    "from groot_imitation.groot_algo.misc_utils import overlay_xmem_mask_on_image, add_palette_on_mask, VideoWriter, get_transformed_depth_img\n",
    "from groot_imitation.groot_algo.o3d_modules import O3DPointCloud, convert_convention\n",
    "\n",
    "\n",
    "from IPython.display import Video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of XMem VOS\n",
    "In this part, you will learn how to use:\n",
    "1. xmem_tracker wrapper that makes it easy to process video streams in robotics domains\n",
    "2. how to render videos with the VOS masks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = f\"{project_repo_folder}/datasets/annotations/example_demo\"\n",
    "demo_file_name = f\"{project_repo_folder}/datasets/example_demo.hdf5\"\n",
    "\n",
    "first_frame, first_frame_annotation = get_first_frame_annotation(annotation_folder)\n",
    "\n",
    "# ************************ Most important part *******************************\n",
    "xmem_tracker = XMemTracker(xmem_checkpoint=f'{project_repo_folder}/third_party/xmem_checkpoints/XMem.pth', device='cuda:0')\n",
    "xmem_tracker.clear_memory()\n",
    "# **************************************************************************\n",
    "\n",
    "resized_images = []\n",
    "\n",
    "with h5py.File(demo_file_name, \"r\") as f:\n",
    "    images = f[\"data/demo_0/obs\"][\"agentview_rgb\"][:]\n",
    "\n",
    "for image in images:\n",
    "    image = cv2.resize(image, (first_frame_annotation.shape[1], first_frame_annotation.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "    resized_images.append(image)\n",
    "\n",
    "masks = xmem_tracker.track_video(resized_images, first_frame_annotation)\n",
    "\n",
    "\n",
    "mask_file = os.path.join(annotation_folder, \"video_masks.hdf5\")\n",
    "\n",
    "with h5py.File(mask_file, \"w\") as f:\n",
    "    f.create_group(\"data\")\n",
    "    f[\"data\"].create_dataset(\"agentview_masks\", data=np.stack(masks, axis=0))\n",
    "\n",
    "with VideoWriter(video_path=annotation_folder, video_name=\"mask_only_video.mp4\", fps=20, save_video=True) as video_writer:\n",
    "    for mask, image in zip(masks, resized_images):\n",
    "        new_mask_img = add_palette_on_mask(mask).convert(\"RGB\")\n",
    "        video_writer.append_image(np.array(new_mask_img))\n",
    "\n",
    "with VideoWriter(video_path=annotation_folder, video_name=\"overlay_video.mp4\", fps=20, save_video=True) as video_writer:\n",
    "    for mask, image in zip(masks, resized_images):\n",
    "        new_mask_img = overlay_xmem_mask_on_image(image, mask, use_white_bg=True)\n",
    "        video_writer.append_image(np.array(new_mask_img))\n",
    "\n",
    "# Video(os.path.join(annotation_folder, \"overlay_video.mp4\"), embed=True, width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of RGB-D reconstruction\n",
    "In this part, you will learn how to use:\n",
    "1. read in RGB-D image from an example dataset\n",
    "2. reconstruct the point clouds of the images\n",
    "3. load in the segmentation mask, reconstruct object-centric 3d point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_demo_file = f\"{project_repo_folder}/datasets/example_demo.hdf5\"\n",
    "\n",
    "print(\"Reading the example demo file...\")\n",
    "idx = 0\n",
    "with h5py.File(example_demo_file) as f:\n",
    "    # convert_convention fucntion is to make sure that the arrays are saved as contiguous arrays, which is important to make rendering proper.\n",
    "    rgb_image = convert_convention(f[\"data/demo_0/obs\"][\"agentview_rgb\"][idx])\n",
    "    depth_image = convert_convention(f[\"data/demo_0/obs\"][\"agentview_depth\"][idx])\n",
    "    mask_image = f[\"data/demo_0/obs\"][\"agentview_masks\"][idx]\n",
    "    camera_extrinsics = f[\"data/demo_0/obs\"][\"agentview_extrinsics\"][idx]\n",
    "    camera_intrinsics = json.loads(f[\"data\"].attrs[\"camera_intrinsics\"])[\"agentview\"]\n",
    "    print(f[\"data\"].attrs.keys())\n",
    "    print(f[\"data/demo_0/obs\"].keys())\n",
    "mask_image = resize_image_to_same_shape(mask_image, rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_image_in_rgb = depth_to_rgb(depth_image, colormap=\"jet\") # You can visualize the depth with either jet, magma, viridis color map\n",
    "# display the two images by simply concatenating them\n",
    "plt.imshow(np.concatenate([rgb_image, depth_image_in_rgb], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize their 3d point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render 3d point cloud\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "depth_pc = O3DPointCloud()\n",
    "\n",
    "depth_pc.create_from_depth(depth_image, camera_intrinsics)\n",
    "\n",
    "depth_pc.transform(camera_extrinsics)\n",
    "point_cloud = depth_pc.get_points()\n",
    "x_vals = point_cloud[:, 0]\n",
    "y_vals = point_cloud[:, 1]\n",
    "z_vals = point_cloud[:, 2]\n",
    "\n",
    "# Create the scatter3d plot\n",
    "scatter = go.Scatter3d(\n",
    "    x=x_vals,\n",
    "    y=y_vals,\n",
    "    z=z_vals,\n",
    "    mode='markers',\n",
    "    marker=dict(size=3, color=z_vals, colorscale='Viridis', opacity=0.8)\n",
    ")\n",
    "\n",
    "# Set the layout for the plot\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "# Combine the scatter plot and layout to create a figure\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbd_pc = O3DPointCloud()\n",
    "rgbd_pc.create_from_rgbd(rgb_image, depth_image, camera_intrinsics)\n",
    "rgbd_pc.transform(camera_extrinsics)\n",
    "\n",
    "point_cloud = rgbd_pc.get_points()\n",
    "colors_rgb = rgbd_pc.get_colors()\n",
    "# Convert RGB colors to a format recognizable by Plotly\n",
    "color_str = ['rgb('+str(r)+','+str(g)+','+str(b)+')' for r,g,b in colors_rgb]\n",
    "\n",
    "# Extract x, y, and z columns from the point cloud\n",
    "x_vals = point_cloud[:, 0]\n",
    "y_vals = point_cloud[:, 1]\n",
    "z_vals = point_cloud[:, 2]\n",
    "\n",
    "# Create the scatter3d plot\n",
    "rgbd_scatter = go.Scatter3d(\n",
    "    x=x_vals,\n",
    "    y=y_vals,\n",
    "    z=z_vals,\n",
    "    mode='markers',\n",
    "    marker=dict(size=3, color=color_str, opacity=0.8)\n",
    ")\n",
    "\n",
    "# Set the layout for the plot\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[rgbd_scatter], layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "object_centric_scatters = []\n",
    "for mask_idx in range(1, mask_image.max()):\n",
    "    # crop the image and depth\n",
    "    masked_depth_image = depth_image.copy()\n",
    "    binary_mask = np.where(mask_image==mask_idx, 1, 0)\n",
    "    masked_depth_image[binary_mask == 0] = -1\n",
    "\n",
    "    object_pcd = O3DPointCloud()\n",
    "    object_pcd.create_from_rgbd(rgb_image, masked_depth_image, camera_intrinsics)\n",
    "    object_pcd.transform(camera_extrinsics)\n",
    "    # object_pcd.preprocess()\n",
    "\n",
    "    point_cloud = object_pcd.get_points()\n",
    "    colors_rgb = object_pcd.get_colors()\n",
    "\n",
    "    x_vals = point_cloud[:, 0]\n",
    "    y_vals = point_cloud[:, 1]\n",
    "    z_vals = point_cloud[:, 2]\n",
    "\n",
    "    # Convert RGB colors to a format recognizable by Plotly\n",
    "    color_str = ['rgb('+str(r)+','+str(g)+','+str(b)+')' for r,g,b in colors_rgb]\n",
    "    scatter = go.Scatter3d(\n",
    "        x=x_vals,\n",
    "        y=y_vals,\n",
    "        z=z_vals,\n",
    "        mode='markers',\n",
    "        marker=dict(size=3, color=color_str, opacity=0.8)\n",
    "    )\n",
    "    object_centric_scatters.append(scatter)\n",
    "\n",
    "\n",
    "# Set the layout for the plot\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=object_centric_scatters, layout=layout)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera perspective augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_camera_extrinsics = rotate_camera_pose(camera_extrinsics, angle=-60, point=[0.6, 0, 0])\n",
    "\n",
    "new_depth_img, z_max = get_transformed_depth_img(\n",
    "    point_cloud=rgbd_pc.get_points(),\n",
    "    camera_intrinsics=np.array(camera_intrinsics),\n",
    "    new_camera_extrinsics=new_camera_extrinsics,\n",
    "    camera_width=224,\n",
    "    camera_height=224,\n",
    ")\n",
    "\n",
    "plt.imshow(depth_to_rgb(new_depth_img, colormap=\"jet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Segment Correspondence Model\n",
    "In this part, you will learn how to use:\n",
    "1. get a segmentation of the image using SAM\n",
    "2. load a reference image, and compute DINOv2 feature using our DINOv2 wrapper\n",
    "3. interactively visualize DINOv2 cost volume (may need to use jupyter-dash)\n",
    "4. get the correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groot_imitation.groot_algo.sam_operator import SAMOperator\n",
    "from groot_imitation.groot_algo.dino_features import DinoV2ImageProcessor, compute_affinity, rescale_feature_map, generate_video_from_affinity\n",
    "\n",
    "dinov2 = DinoV2ImageProcessor()\n",
    "sam_operator = SAMOperator()\n",
    "sam_operator.print_config()\n",
    "sam_operator.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "# autocast_dtype = torch.half\n",
    "# autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=autocast_dtype)\n",
    "# with autocast_ctx():\n",
    "#     with torch.no_grad():\n",
    "mask_result_dict = sam_operator.segment_image(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visaualize the mask using plotly\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "overall_mask = (mask_result_dict[\"overall_mask\"] * 255).astype(np.uint8)\n",
    "mask_ids = mask_result_dict[\"merged_mask\"]\n",
    "# draw the mask image in plotly canvas\n",
    "fig = px.imshow(overall_mask)\n",
    "\n",
    "fig.data[0].customdata = mask_ids\n",
    "# fig.data[0].hovertemplate = '<b>Mask ID:</b> %{customdata}'\n",
    "fig.data[0].hovertemplate = 'x: %{x}<br>y: %{y}<br>Mask ID: %{customdata}'\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    showlegend=False,\n",
    "    width=300,   # you can adjust this as needed\n",
    "    height=300,   # you can adjust this as needed\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_instance_image = np.array(Image.open(os.path.join(GROOT_ROOT_PATH, \"../\", \"datasets\", \"example_new_object.jpg\")))\n",
    "new_instance_image = resize_image_to_same_shape(new_instance_image, rgb_image)\n",
    "\n",
    "img_list = []\n",
    "feature_list = []\n",
    "for img in [rgb_image, new_instance_image]:\n",
    "    img_list.append(img)\n",
    "    feature_list.append(dinov2.process_image(img))\n",
    "\n",
    "saved_video_file = generate_video_from_affinity(\n",
    "    img_list[0], \n",
    "    img_list[1], \n",
    "    feature_list[0], \n",
    "    feature_list[1],\n",
    "    h=32,\n",
    "    w=32,\n",
    "    patch_size=14,\n",
    "    )\n",
    "# display the video\n",
    "# Video(saved_video_file, embed=True, width=500, height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCM Model example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groot_imitation.segmentation_correspondence_model.scm import SegmentationCorrespondenceModel\n",
    "\n",
    "scm_module = SegmentationCorrespondenceModel(dinov2=dinov2, sam_operator=sam_operator)\n",
    "\n",
    "new_annotation_mask = scm_module(new_instance_image, rgb_image, mask_image)\n",
    "new_annotation_mask = resize_image_to_same_shape(new_annotation_mask, new_instance_image)\n",
    "print(new_instance_image.shape, new_annotation_mask.shape)\n",
    "new_instance_overlay_image = overlay_xmem_mask_on_image(new_instance_image, new_annotation_mask, use_white_bg=True)\n",
    "\n",
    "plotly_draw_seg_image(new_instance_overlay_image, new_annotation_mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-continual-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
